{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "e_H2P_etFLw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qvk1Btc4BIov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pls\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# cd '/content/drive/My Drive/Crop Disease Recognition'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "hw2Q9e35BJrD",
        "outputId": "0dbc80da-0786-4ea0-a7aa-daa76c1b19ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d626bdbc0ae7>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    cd '/content/drive/My Drive/Crop Disease Recognition'\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load Images\n",
        " "
      ],
      "metadata": {
        "id": "SRRgMrCDEvW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator (\n",
        "rescale = 1/255.0\n",
        ") # Creating an object, that just hold the data, 1 image.\n",
        "train_images = train_data_gen.flow_from_directory(\n",
        "# flow_from_directory/ is a method : will help to execute / instruct to go\n",
        "# following folder\n",
        " directory='drive/MyDrive/DL-Econic/Crop Disease/AugmentedData',\n",
        "# because dl wants more data\n",
        "batch_size = 32,\n",
        "target_size = (228, 228)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "zm7IkV3sBQiW",
        "outputId": "9a1729c5-a89b-4717-f9f9-b49301a9d796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-08f8f9e172da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ) # Creating an object, that just hold the data, 1 image.\n\u001b[0;32m----> 4\u001b[0;31m train_images = train_data_gen.flow_from_directory(\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# flow_from_directory/ is a method : will help to execute / instruct to go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# following folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/DL-Econic/Crop Disease/AugmentedData'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* each folder will be each category & each category will holds its own image\n",
        "* target_size: I dont know the actual size of image, that why i declared to read the image with\n",
        "228 by 228 in pixel size / height & width\n",
        "color not defined, because whatever the color machine will understand\n",
        "* code 5-8/ method: Return a things that is generator\n",
        "* Now the data is fridge, when executes it will call one by one, do not load whole data at a\n",
        "time, applied batch, not keeping all together, that helps for processing\n",
        "* train_test_gen is object & flow_from_directory is method, method applied on object\n",
        "* 3 classess means 3 folders\n"
      ],
      "metadata": {
        "id": "fhhPRtLSb0nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data_gen = tf.keras.preprocessing.image.ImageDataGenerator (\n",
        "rescale = 1 / 255.0 \n",
        "# 1/255 works little better, any value can be applied, depending dataset can be differet, 1 mean 1 imamge\n",
        ")\n",
        "valid_images = valid_data_gen.flow_from_directory (\n",
        "'drive/MyDrive/DL-Econic/Crop Disease/data',\n",
        "batch_size = 24, \n",
        "# total 120 image, which I want to keep in 5 batch so divided by 24\n",
        "target_size = (228, 228)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "iksYegRVBXf_",
        "outputId": "bb420ead-6402-49ad-81d0-1b0a81ed2d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-411510b64431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1/255 works little better, any value can be applied, depending dataset can be differet, 1 mean 1 imamge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0;31m valid_images = valid_data_gen.flow_from_directory (\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m'drive/MyDrive/DL-Econic/Crop Disease/data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/DL-Econic/Crop Disease/data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generting Batch by Next Function**\n"
      ],
      "metadata": {
        "id": "SwUWsGfYcXaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its return two: 1 is image: 0 & label of that batch or batch_size when index 1"
      ],
      "metadata": {
        "id": "b5PCGIWmcdMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(valid_images)[1] \n",
        "# Labels/ class Stored when index 1, see everything is ready, all data are in properly one hot encoded vector shape, its save more time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "HlgCDDYACWIB",
        "outputId": "d97e3233-008d-46b1-924c-ae5ddbda383d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9f905a1c6863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Labels/ class Stored when index 1, see everything is ready, all data are in properly one hot encoded vector shape, its save more time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(valid_images)[0] # Image Stored when index 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "oxYDWAl0C1b4",
        "outputId": "a8ad218f-988e-4015-d052-8b8a19ff3b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e0785c77dc6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Image Stored when index 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Method / Function**"
      ],
      "metadata": {
        "id": "UnXVWjueck--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Creating Method for ploting “Loss” & “Accuracy”\n",
        "*  It will take from history & training\n",
        "*  plot for trainin loss or validation loss / error \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xxiJErdScpxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotLoss(history:pd.DataFrame):\n",
        "# Providing history dataframe, which will keep training history in neuron network & see that over the time how model learned graphically\n",
        "  plt.figure(figsize=(18,20))\n",
        "  plt.plot(history.index.values,\n",
        "  history['loss'], label='Training Error', color = 'darkorange',linewidth=3)\n",
        "  plt.plot(history.index.values,\n",
        "  history['val_loss'], label = 'Validation Error', color ='lightgreen', linewidth=3)\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "def plotAccuracy(history:pd.DataFrame):\n",
        " plt.figure(figsize=(18,20))\n",
        " plt.plot(history.index.values, history['accuracy'],\n",
        " label='Training Accuracy', color = 'darkorange', linewidth=3)\n",
        " plt.plot(history.index.values, history['val_accuracy'],\n",
        " label='Validation Accuracy', color='lightgreen', linewidth=3)\n",
        " plt.legend()\n",
        " plt.grid(True)\n",
        " plt.show()\n"
      ],
      "metadata": {
        "id": "dFBnfm5vC9gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize - CNN**"
      ],
      "metadata": {
        "id": "CcpQF67zdcwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.Sequential([\n",
        "# Convolution Feature Extraction # First layer is convolutional layer, normalization can also be applied, its optional\n",
        "tf.keras.layers.Conv2D( \n",
        "# There are also 1D,2D & 3D available\n",
        "filters=32, kernel_size=(3,3),\n",
        "strides=(1,1), activation='relu',\n",
        "input_shape=(228, 228, 3), padding='same' \n",
        "# Since its first layer, its dont know enough, so input spape must be defined, so that it can understand\n",
        "),\n",
        "# Dimesionality Reduction # After 1st conv, there are so many dimenstion, that must be reduced\n",
        "tf.keras.layers.MaxPool2D( \n",
        "# Maxpool 2D, becuause our filter/ Cov2D, so its must be 2D\n",
        "pool_size=(2,2), strides=(1,1), padding='same'\n",
        "),\n",
        "# Batch Normalization # Batch Normalization works better for image processing\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "# after normalization, still its in image for, so need to covert into 1D array\n",
        "tf.keras.layers.Flatten(), \n",
        "# Flatting for converting 1D array\n",
        "# fulluy Connected Network # Connecting Artifician Neuron Network\n",
        "tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "tf.keras.layers.Dropout(rate=0.5, seed=42), #regularization: dropout, earlystopping, cotroll randomness, seed 42\n",
        "tf.keras.layers.Dense(units=3, activation='softmax') \n",
        "# 3 for 3 class classfication / for multiclass softmax\n",
        "])\n"
      ],
      "metadata": {
        "id": "29QnPTRmDihR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilation"
      ],
      "metadata": {
        "id": "lIfccrJwdh_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(\n",
        "optimizer = 'adam',\n",
        "loss = 'categorical_crossentropy', # Categorical crosentropy for calculating loss for multiclass\n",
        "metrics = ['accuracy'] # Matrics for evaluate that is accuracy\n",
        ")\n"
      ],
      "metadata": {
        "id": "-wf-uf4-D6Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU Activation**"
      ],
      "metadata": {
        "id": "KUNKnftTm286"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go Runtime > Change runtime type > hardware > GPU » run all"
      ],
      "metadata": {
        "id": "VvL2cilCm71o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.list_physical_devices() \n",
        "# Check the CPU & GPU Status, here I have 1 CPU & 1 GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gApE1ATEPsS",
        "outputId": "a9758bd5-65f9-48b5-e515-92158c3c9f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the Model - with GPU"
      ],
      "metadata": {
        "id": "-Cb_--d7m_b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'): # Creating Session: ask to use GPU devise to perform all the follwing opertion\n",
        " cnn.fit(\n",
        "   train_images, # Generator\n",
        "   epochs= 25,\n",
        "   validation_data = valid_images # Generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "XR33pHLXETmu",
        "outputId": "b5ef8d35-4965-41d4-e09b-061d6d3a6871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8adf01c1ee82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Creating Session: ask to use GPU devise to perform all the follwing opertion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m  cnn.fit(\n\u001b[0;32m----> 3\u001b[0;31m    \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_images\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Summary - CNN**"
      ],
      "metadata": {
        "id": "McCadG1anEeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751r3kRsEXyv",
        "outputId": "adf2baec-ab23-42a0-9ee2-7709e4099c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 228, 228, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 228, 228, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 228, 228, 32)     128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1663488)           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               212926592 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 212,928,003\n",
            "Trainable params: 212,927,939\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = pd.DataFrame(cnn.history.history)\n",
        "hist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "kLrNdgjbEjcv",
        "outputId": "c89d58f3-570e-4602-cf21-f81d301ccd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2ee8c30dd64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Used frameworks like Tensorflow,Keras and python libraries like Numpy, Pandas and MatPlotlib for data training and preprocessing.\n",
        "CNN is made to train this model and get an accuracy of 80.83 on testing dataset"
      ],
      "metadata": {
        "id": "lM0Jw2iwNdeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}